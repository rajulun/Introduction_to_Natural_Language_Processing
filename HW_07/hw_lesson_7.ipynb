{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw_lesson_7.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "323xVlXU7asI",
        "colab_type": "text"
      },
      "source": [
        "## Урок 7. Модель Transformer.\n",
        "\n",
        "Запустить seq2seq, seq2seq с внимаием и трансформер для перевода русских слов + описать наблюдения по качеству\n",
        "Данные в папке data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WUZZYUxMkr4",
        "colab_type": "text"
      },
      "source": [
        "seq2seq"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Tfd9qZY7YIZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-HxJYtn9lmN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 128\n",
        "epochs = 100\n",
        "latent_dim = 256\n",
        "num_samples = 10000\n",
        "data_path = 'rus.txt'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHkOLSUo9ljn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Собираем из текстов токены и делаем pne-hot вектора на каждый токен\n",
        "\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    input_text, target_text, _ = line.split('\\t')\n",
        "    target_text = '\\t' + target_text + '\\n'\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in input_characters:\n",
        "            input_characters.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)\n",
        "\n",
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xY0DVZpl9lg7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(target_characters)])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgJ1kySf9ld2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiKj7KWd9lbT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
        "    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n",
        "    for t, char in enumerate(target_text):\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
        "        if t > 0:\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
        "    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n",
        "    decoder_target_data[i, t:, target_token_index[' ']] = 1."
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WazSoVB92YQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "078f4d81-6abc-419e-ae04-c4654fc82060"
      },
      "source": [
        "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "encoder = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
        "                                     initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_split=0.2)\n",
        "\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states)\n",
        "\n",
        "reverse_input_char_index = dict(\n",
        "    (i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict(\n",
        "    (i, char) for char, i in target_token_index.items())\n",
        "\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value)\n",
        "\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        if (sampled_char == '\\n' or\n",
        "           len(decoded_sentence) > max_decoder_seq_length):\n",
        "            stop_condition = True\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence\n",
        "\n",
        "\n",
        "for seq_index in range(100):\n",
        "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', input_texts[seq_index])\n",
        "    print('Decoded sentence:', decoded_sentence)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "63/63 [==============================] - 2s 25ms/step - loss: 1.3889 - accuracy: 0.7636 - val_loss: 1.0451 - val_accuracy: 0.7550\n",
            "Epoch 2/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.8525 - accuracy: 0.7819 - val_loss: 0.9360 - val_accuracy: 0.7596\n",
            "Epoch 3/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.7600 - accuracy: 0.7916 - val_loss: 0.8266 - val_accuracy: 0.7833\n",
            "Epoch 4/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.6965 - accuracy: 0.8173 - val_loss: 0.7702 - val_accuracy: 0.7943\n",
            "Epoch 5/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.6422 - accuracy: 0.8304 - val_loss: 0.7125 - val_accuracy: 0.8106\n",
            "Epoch 6/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.5943 - accuracy: 0.8403 - val_loss: 0.6666 - val_accuracy: 0.8184\n",
            "Epoch 7/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.5586 - accuracy: 0.8464 - val_loss: 0.6361 - val_accuracy: 0.8221\n",
            "Epoch 8/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.5348 - accuracy: 0.8504 - val_loss: 0.6101 - val_accuracy: 0.8268\n",
            "Epoch 9/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.5158 - accuracy: 0.8541 - val_loss: 0.5925 - val_accuracy: 0.8305\n",
            "Epoch 10/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.4997 - accuracy: 0.8578 - val_loss: 0.5780 - val_accuracy: 0.8328\n",
            "Epoch 11/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.4872 - accuracy: 0.8606 - val_loss: 0.5640 - val_accuracy: 0.8378\n",
            "Epoch 12/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.4736 - accuracy: 0.8637 - val_loss: 0.5505 - val_accuracy: 0.8404\n",
            "Epoch 13/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.4625 - accuracy: 0.8666 - val_loss: 0.5398 - val_accuracy: 0.8428\n",
            "Epoch 14/100\n",
            "63/63 [==============================] - 1s 16ms/step - loss: 0.4510 - accuracy: 0.8699 - val_loss: 0.5329 - val_accuracy: 0.8462\n",
            "Epoch 15/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.4415 - accuracy: 0.8726 - val_loss: 0.5226 - val_accuracy: 0.8492\n",
            "Epoch 16/100\n",
            "63/63 [==============================] - 1s 16ms/step - loss: 0.4322 - accuracy: 0.8749 - val_loss: 0.5184 - val_accuracy: 0.8504\n",
            "Epoch 17/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.4235 - accuracy: 0.8775 - val_loss: 0.5126 - val_accuracy: 0.8522\n",
            "Epoch 18/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.4157 - accuracy: 0.8795 - val_loss: 0.5071 - val_accuracy: 0.8528\n",
            "Epoch 19/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.4091 - accuracy: 0.8814 - val_loss: 0.4982 - val_accuracy: 0.8560\n",
            "Epoch 20/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.4002 - accuracy: 0.8840 - val_loss: 0.4939 - val_accuracy: 0.8580\n",
            "Epoch 21/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.3934 - accuracy: 0.8859 - val_loss: 0.4889 - val_accuracy: 0.8590\n",
            "Epoch 22/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.3864 - accuracy: 0.8879 - val_loss: 0.4876 - val_accuracy: 0.8595\n",
            "Epoch 23/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.3810 - accuracy: 0.8895 - val_loss: 0.4803 - val_accuracy: 0.8616\n",
            "Epoch 24/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.3726 - accuracy: 0.8921 - val_loss: 0.4762 - val_accuracy: 0.8619\n",
            "Epoch 25/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.3666 - accuracy: 0.8935 - val_loss: 0.4755 - val_accuracy: 0.8634\n",
            "Epoch 26/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.3598 - accuracy: 0.8953 - val_loss: 0.4723 - val_accuracy: 0.8650\n",
            "Epoch 27/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.3541 - accuracy: 0.8969 - val_loss: 0.4720 - val_accuracy: 0.8647\n",
            "Epoch 28/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.3478 - accuracy: 0.8987 - val_loss: 0.4647 - val_accuracy: 0.8663\n",
            "Epoch 29/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.3420 - accuracy: 0.9002 - val_loss: 0.4635 - val_accuracy: 0.8672\n",
            "Epoch 30/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.3356 - accuracy: 0.9020 - val_loss: 0.4616 - val_accuracy: 0.8683\n",
            "Epoch 31/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.3305 - accuracy: 0.9037 - val_loss: 0.4610 - val_accuracy: 0.8685\n",
            "Epoch 32/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.3242 - accuracy: 0.9055 - val_loss: 0.4632 - val_accuracy: 0.8676\n",
            "Epoch 33/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.3190 - accuracy: 0.9069 - val_loss: 0.4575 - val_accuracy: 0.8708\n",
            "Epoch 34/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.3131 - accuracy: 0.9084 - val_loss: 0.4561 - val_accuracy: 0.8703\n",
            "Epoch 35/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.3078 - accuracy: 0.9098 - val_loss: 0.4601 - val_accuracy: 0.8697\n",
            "Epoch 36/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.3022 - accuracy: 0.9116 - val_loss: 0.4570 - val_accuracy: 0.8714\n",
            "Epoch 37/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.2997 - accuracy: 0.9122 - val_loss: 0.4509 - val_accuracy: 0.8728\n",
            "Epoch 38/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.2910 - accuracy: 0.9146 - val_loss: 0.4535 - val_accuracy: 0.8734\n",
            "Epoch 39/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.2866 - accuracy: 0.9161 - val_loss: 0.4554 - val_accuracy: 0.8732\n",
            "Epoch 40/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.2805 - accuracy: 0.9182 - val_loss: 0.4518 - val_accuracy: 0.8741\n",
            "Epoch 41/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.2755 - accuracy: 0.9193 - val_loss: 0.4523 - val_accuracy: 0.8742\n",
            "Epoch 42/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.2707 - accuracy: 0.9208 - val_loss: 0.4528 - val_accuracy: 0.8750\n",
            "Epoch 43/100\n",
            "63/63 [==============================] - 1s 14ms/step - loss: 0.2658 - accuracy: 0.9220 - val_loss: 0.4532 - val_accuracy: 0.8744\n",
            "Epoch 44/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.2616 - accuracy: 0.9232 - val_loss: 0.4594 - val_accuracy: 0.8742\n",
            "Epoch 45/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.2564 - accuracy: 0.9248 - val_loss: 0.4560 - val_accuracy: 0.8747\n",
            "Epoch 46/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.2519 - accuracy: 0.9260 - val_loss: 0.4511 - val_accuracy: 0.8767\n",
            "Epoch 47/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.2453 - accuracy: 0.9280 - val_loss: 0.4576 - val_accuracy: 0.8755\n",
            "Epoch 48/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.2416 - accuracy: 0.9291 - val_loss: 0.4566 - val_accuracy: 0.8759\n",
            "Epoch 49/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.2362 - accuracy: 0.9303 - val_loss: 0.4608 - val_accuracy: 0.8749\n",
            "Epoch 50/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.2314 - accuracy: 0.9318 - val_loss: 0.4587 - val_accuracy: 0.8753\n",
            "Epoch 51/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.2272 - accuracy: 0.9330 - val_loss: 0.4616 - val_accuracy: 0.8759\n",
            "Epoch 52/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.2233 - accuracy: 0.9341 - val_loss: 0.4653 - val_accuracy: 0.8758\n",
            "Epoch 53/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.2182 - accuracy: 0.9356 - val_loss: 0.4641 - val_accuracy: 0.8756\n",
            "Epoch 54/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.2137 - accuracy: 0.9367 - val_loss: 0.4677 - val_accuracy: 0.8758\n",
            "Epoch 55/100\n",
            "63/63 [==============================] - 1s 14ms/step - loss: 0.2106 - accuracy: 0.9376 - val_loss: 0.4689 - val_accuracy: 0.8754\n",
            "Epoch 56/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.2096 - accuracy: 0.9380 - val_loss: 0.4658 - val_accuracy: 0.8772\n",
            "Epoch 57/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.2016 - accuracy: 0.9402 - val_loss: 0.4689 - val_accuracy: 0.8768\n",
            "Epoch 58/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.1969 - accuracy: 0.9417 - val_loss: 0.4720 - val_accuracy: 0.8774\n",
            "Epoch 59/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.1925 - accuracy: 0.9429 - val_loss: 0.4755 - val_accuracy: 0.8770\n",
            "Epoch 60/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.1882 - accuracy: 0.9441 - val_loss: 0.4800 - val_accuracy: 0.8763\n",
            "Epoch 61/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.1837 - accuracy: 0.9455 - val_loss: 0.4800 - val_accuracy: 0.8770\n",
            "Epoch 62/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.1799 - accuracy: 0.9465 - val_loss: 0.4882 - val_accuracy: 0.8752\n",
            "Epoch 63/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.1762 - accuracy: 0.9475 - val_loss: 0.4918 - val_accuracy: 0.8752\n",
            "Epoch 64/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.1728 - accuracy: 0.9488 - val_loss: 0.4941 - val_accuracy: 0.8755\n",
            "Epoch 65/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.1690 - accuracy: 0.9497 - val_loss: 0.4947 - val_accuracy: 0.8764\n",
            "Epoch 66/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.1646 - accuracy: 0.9509 - val_loss: 0.4982 - val_accuracy: 0.8767\n",
            "Epoch 67/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.1604 - accuracy: 0.9522 - val_loss: 0.5011 - val_accuracy: 0.8763\n",
            "Epoch 68/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.1577 - accuracy: 0.9530 - val_loss: 0.5013 - val_accuracy: 0.8757\n",
            "Epoch 69/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.1536 - accuracy: 0.9541 - val_loss: 0.5064 - val_accuracy: 0.8760\n",
            "Epoch 70/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.1495 - accuracy: 0.9554 - val_loss: 0.5130 - val_accuracy: 0.8750\n",
            "Epoch 71/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.1469 - accuracy: 0.9561 - val_loss: 0.5126 - val_accuracy: 0.8763\n",
            "Epoch 72/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.1434 - accuracy: 0.9572 - val_loss: 0.5219 - val_accuracy: 0.8755\n",
            "Epoch 73/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.1396 - accuracy: 0.9582 - val_loss: 0.5256 - val_accuracy: 0.8762\n",
            "Epoch 74/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.1401 - accuracy: 0.9578 - val_loss: 0.5241 - val_accuracy: 0.8758\n",
            "Epoch 75/100\n",
            "63/63 [==============================] - 1s 14ms/step - loss: 0.1350 - accuracy: 0.9597 - val_loss: 0.5335 - val_accuracy: 0.8747\n",
            "Epoch 76/100\n",
            "63/63 [==============================] - 1s 14ms/step - loss: 0.1307 - accuracy: 0.9608 - val_loss: 0.5344 - val_accuracy: 0.8744\n",
            "Epoch 77/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.1272 - accuracy: 0.9620 - val_loss: 0.5399 - val_accuracy: 0.8745\n",
            "Epoch 78/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.1239 - accuracy: 0.9629 - val_loss: 0.5411 - val_accuracy: 0.8749\n",
            "Epoch 79/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.1210 - accuracy: 0.9637 - val_loss: 0.5483 - val_accuracy: 0.8740\n",
            "Epoch 80/100\n",
            "63/63 [==============================] - 1s 14ms/step - loss: 0.1187 - accuracy: 0.9641 - val_loss: 0.5533 - val_accuracy: 0.8743\n",
            "Epoch 81/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.1167 - accuracy: 0.9650 - val_loss: 0.5509 - val_accuracy: 0.8738\n",
            "Epoch 82/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.1134 - accuracy: 0.9656 - val_loss: 0.5631 - val_accuracy: 0.8744\n",
            "Epoch 83/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.1109 - accuracy: 0.9663 - val_loss: 0.5638 - val_accuracy: 0.8740\n",
            "Epoch 84/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.1089 - accuracy: 0.9668 - val_loss: 0.5663 - val_accuracy: 0.8749\n",
            "Epoch 85/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.1061 - accuracy: 0.9677 - val_loss: 0.5714 - val_accuracy: 0.8744\n",
            "Epoch 86/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.1030 - accuracy: 0.9685 - val_loss: 0.5752 - val_accuracy: 0.8746\n",
            "Epoch 87/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.1001 - accuracy: 0.9696 - val_loss: 0.5844 - val_accuracy: 0.8732\n",
            "Epoch 88/100\n",
            "63/63 [==============================] - 1s 14ms/step - loss: 0.0985 - accuracy: 0.9700 - val_loss: 0.5868 - val_accuracy: 0.8742\n",
            "Epoch 89/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.0968 - accuracy: 0.9704 - val_loss: 0.5898 - val_accuracy: 0.8738\n",
            "Epoch 90/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.0947 - accuracy: 0.9709 - val_loss: 0.5959 - val_accuracy: 0.8740\n",
            "Epoch 91/100\n",
            "63/63 [==============================] - 1s 14ms/step - loss: 0.0925 - accuracy: 0.9716 - val_loss: 0.6033 - val_accuracy: 0.8729\n",
            "Epoch 92/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.0897 - accuracy: 0.9724 - val_loss: 0.5994 - val_accuracy: 0.8747\n",
            "Epoch 93/100\n",
            "63/63 [==============================] - 1s 16ms/step - loss: 0.0879 - accuracy: 0.9729 - val_loss: 0.6138 - val_accuracy: 0.8726\n",
            "Epoch 94/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.0859 - accuracy: 0.9735 - val_loss: 0.6141 - val_accuracy: 0.8729\n",
            "Epoch 95/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.0838 - accuracy: 0.9740 - val_loss: 0.6213 - val_accuracy: 0.8733\n",
            "Epoch 96/100\n",
            "63/63 [==============================] - 1s 14ms/step - loss: 0.0822 - accuracy: 0.9744 - val_loss: 0.6265 - val_accuracy: 0.8731\n",
            "Epoch 97/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.0807 - accuracy: 0.9747 - val_loss: 0.6259 - val_accuracy: 0.8730\n",
            "Epoch 98/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.0841 - accuracy: 0.9736 - val_loss: 0.6498 - val_accuracy: 0.8689\n",
            "Epoch 99/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.1073 - accuracy: 0.9652 - val_loss: 0.6209 - val_accuracy: 0.8739\n",
            "Epoch 100/100\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.0812 - accuracy: 0.9744 - val_loss: 0.6252 - val_accuracy: 0.8742\n",
            "-\n",
            "Input sentence: <start> Go . <end>\n",
            "Decoded sentence: Иди.\n",
            "\n",
            "-\n",
            "Input sentence: <start> Go . <end>\n",
            "Decoded sentence: Иди.\n",
            "\n",
            "-\n",
            "Input sentence: <start> Go . <end>\n",
            "Decoded sentence: Иди.\n",
            "\n",
            "-\n",
            "Input sentence: <start> Hi . <end>\n",
            "Decoded sentence: Здравствуйте.\n",
            "\n",
            "-\n",
            "Input sentence: <start> Hi . <end>\n",
            "Decoded sentence: Здравствуйте.\n",
            "\n",
            "-\n",
            "Input sentence: <start> Hi . <end>\n",
            "Decoded sentence: Здравствуйте.\n",
            "\n",
            "-\n",
            "Input sentence: <start> Hi . <end>\n",
            "Decoded sentence: Здравствуйте.\n",
            "\n",
            "-\n",
            "Input sentence: <start> Hi . <end>\n",
            "Decoded sentence: Здравствуйте.\n",
            "\n",
            "-\n",
            "Input sentence: <start> Run ! <end>\n",
            "Decoded sentence: Беги!\n",
            "\n",
            "-\n",
            "Input sentence: <start> Run ! <end>\n",
            "Decoded sentence: Беги!\n",
            "\n",
            "-\n",
            "Input sentence: <start> Run . <end>\n",
            "Decoded sentence: Беги!\n",
            "\n",
            "-\n",
            "Input sentence: <start> Run . <end>\n",
            "Decoded sentence: Беги!\n",
            "\n",
            "-\n",
            "Input sentence: <start> Who ? <end>\n",
            "Decoded sentence: Кто?\n",
            "\n",
            "-\n",
            "Input sentence: <start> Wow ! <end>\n",
            "Decoded sentence: Вот это!\n",
            "\n",
            "-\n",
            "Input sentence: <start> Wow ! <end>\n",
            "Decoded sentence: Вот это!\n",
            "\n",
            "-\n",
            "Input sentence: <start> Wow ! <end>\n",
            "Decoded sentence: Вот это!\n",
            "\n",
            "-\n",
            "Input sentence: <start> Wow ! <end>\n",
            "Decoded sentence: Вот это!\n",
            "\n",
            "-\n",
            "Input sentence: <start> Wow ! <end>\n",
            "Decoded sentence: Вот это!\n",
            "\n",
            "-\n",
            "Input sentence: <start> Wow ! <end>\n",
            "Decoded sentence: Вот это!\n",
            "\n",
            "-\n",
            "Input sentence: <start> Fire ! <end>\n",
            "Decoded sentence: Огонь!\n",
            "\n",
            "-\n",
            "Input sentence: <start> Fire ! <end>\n",
            "Decoded sentence: Огонь!\n",
            "\n",
            "-\n",
            "Input sentence: <start> Help ! <end>\n",
            "Decoded sentence: На помощь!\n",
            "\n",
            "-\n",
            "Input sentence: <start> Help ! <end>\n",
            "Decoded sentence: На помощь!\n",
            "\n",
            "-\n",
            "Input sentence: <start> Help ! <end>\n",
            "Decoded sentence: На помощь!\n",
            "\n",
            "-\n",
            "Input sentence: <start> Jump ! <end>\n",
            "Decoded sentence: Прыгай!\n",
            "\n",
            "-\n",
            "Input sentence: <start> Jump ! <end>\n",
            "Decoded sentence: Прыгай!\n",
            "\n",
            "-\n",
            "Input sentence: <start> Jump . <end>\n",
            "Decoded sentence: Прыгай!\n",
            "\n",
            "-\n",
            "Input sentence: <start> Jump . <end>\n",
            "Decoded sentence: Прыгай!\n",
            "\n",
            "-\n",
            "Input sentence: <start> Stop ! <end>\n",
            "Decoded sentence: Остановись!\n",
            "\n",
            "-\n",
            "Input sentence: <start> Stop ! <end>\n",
            "Decoded sentence: Остановись!\n",
            "\n",
            "-\n",
            "Input sentence: <start> Stop ! <end>\n",
            "Decoded sentence: Остановись!\n",
            "\n",
            "-\n",
            "Input sentence: <start> Wait ! <end>\n",
            "Decoded sentence: Подожди!\n",
            "\n",
            "-\n",
            "Input sentence: <start> Wait ! <end>\n",
            "Decoded sentence: Подожди!\n",
            "\n",
            "-\n",
            "Input sentence: <start> Wait ! <end>\n",
            "Decoded sentence: Подожди!\n",
            "\n",
            "-\n",
            "Input sentence: <start> Wait ! <end>\n",
            "Decoded sentence: Подожди!\n",
            "\n",
            "-\n",
            "Input sentence: <start> Wait . <end>\n",
            "Decoded sentence: Подожди.\n",
            "\n",
            "-\n",
            "Input sentence: <start> Wait . <end>\n",
            "Decoded sentence: Подожди.\n",
            "\n",
            "-\n",
            "Input sentence: <start> Wait . <end>\n",
            "Decoded sentence: Подожди.\n",
            "\n",
            "-\n",
            "Input sentence: <start> Do it . <end>\n",
            "Decoded sentence: Сделай это.\n",
            "\n",
            "-\n",
            "Input sentence: <start> Go on . <end>\n",
            "Decoded sentence: Продолжай.\n",
            "\n",
            "-\n",
            "Input sentence: <start> Go on . <end>\n",
            "Decoded sentence: Продолжай.\n",
            "\n",
            "-\n",
            "Input sentence: <start> Hello ! <end>\n",
            "Decoded sentence: Здравствуйте.\n",
            "\n",
            "-\n",
            "Input sentence: <start> Hello ! <end>\n",
            "Decoded sentence: Здравствуйте.\n",
            "\n",
            "-\n",
            "Input sentence: <start> Hello ! <end>\n",
            "Decoded sentence: Здравствуйте.\n",
            "\n",
            "-\n",
            "Input sentence: <start> Hello ! <end>\n",
            "Decoded sentence: Здравствуйте.\n",
            "\n",
            "-\n",
            "Input sentence: <start> Hurry ! <end>\n",
            "Decoded sentence: Поспешите.\n",
            "\n",
            "-\n",
            "Input sentence: <start> I ran . <end>\n",
            "Decoded sentence: Я бежал.\n",
            "\n",
            "-\n",
            "Input sentence: <start> I ran . <end>\n",
            "Decoded sentence: Я бежал.\n",
            "\n",
            "-\n",
            "Input sentence: <start> I ran . <end>\n",
            "Decoded sentence: Я бежал.\n",
            "\n",
            "-\n",
            "Input sentence: <start> I ran . <end>\n",
            "Decoded sentence: Я бежал.\n",
            "\n",
            "-\n",
            "Input sentence: <start> I see . <end>\n",
            "Decoded sentence: Понимаю.\n",
            "\n",
            "-\n",
            "Input sentence: <start> I see . <end>\n",
            "Decoded sentence: Понимаю.\n",
            "\n",
            "-\n",
            "Input sentence: <start> I see . <end>\n",
            "Decoded sentence: Понимаю.\n",
            "\n",
            "-\n",
            "Input sentence: <start> I try . <end>\n",
            "Decoded sentence: Я пробую.\n",
            "\n",
            "-\n",
            "Input sentence: <start> I try . <end>\n",
            "Decoded sentence: Я пробую.\n",
            "\n",
            "-\n",
            "Input sentence: <start> I try . <end>\n",
            "Decoded sentence: Я пробую.\n",
            "\n",
            "-\n",
            "Input sentence: <start> I won ! <end>\n",
            "Decoded sentence: Я победила!\n",
            "\n",
            "-\n",
            "Input sentence: <start> I won ! <end>\n",
            "Decoded sentence: Я победила!\n",
            "\n",
            "-\n",
            "Input sentence: <start> I won ! <end>\n",
            "Decoded sentence: Я победила!\n",
            "\n",
            "-\n",
            "Input sentence: <start> I won ! <end>\n",
            "Decoded sentence: Я победила!\n",
            "\n",
            "-\n",
            "Input sentence: <start> Oh no ! <end>\n",
            "Decoded sentence: О нет!\n",
            "\n",
            "-\n",
            "Input sentence: <start> Relax . <end>\n",
            "Decoded sentence: Расслабьтесь.\n",
            "\n",
            "-\n",
            "Input sentence: <start> Relax . <end>\n",
            "Decoded sentence: Расслабьтесь.\n",
            "\n",
            "-\n",
            "Input sentence: <start> Relax . <end>\n",
            "Decoded sentence: Расслабьтесь.\n",
            "\n",
            "-\n",
            "Input sentence: <start> Shoot ! <end>\n",
            "Decoded sentence: Стрей!\n",
            "\n",
            "-\n",
            "Input sentence: <start> Smile . <end>\n",
            "Decoded sentence: Улыбайтесь!\n",
            "\n",
            "-\n",
            "Input sentence: <start> Smile . <end>\n",
            "Decoded sentence: Улыбайтесь!\n",
            "\n",
            "-\n",
            "Input sentence: <start> Smile . <end>\n",
            "Decoded sentence: Улыбайтесь!\n",
            "\n",
            "-\n",
            "Input sentence: <start> Smile . <end>\n",
            "Decoded sentence: Улыбайтесь!\n",
            "\n",
            "-\n",
            "Input sentence: <start> Smile . <end>\n",
            "Decoded sentence: Улыбайтесь!\n",
            "\n",
            "-\n",
            "Input sentence: <start> Smile . <end>\n",
            "Decoded sentence: Улыбайтесь!\n",
            "\n",
            "-\n",
            "Input sentence: <start> Attack ! <end>\n",
            "Decoded sentence: Отаная!\n",
            "\n",
            "-\n",
            "Input sentence: <start> Cheers ! <end>\n",
            "Decoded sentence: За ваше здоровье!\n",
            "\n",
            "-\n",
            "Input sentence: <start> Cheers ! <end>\n",
            "Decoded sentence: За ваше здоровье!\n",
            "\n",
            "-\n",
            "Input sentence: <start> Cheers ! <end>\n",
            "Decoded sentence: За ваше здоровье!\n",
            "\n",
            "-\n",
            "Input sentence: <start> Cheers ! <end>\n",
            "Decoded sentence: За ваше здоровье!\n",
            "\n",
            "-\n",
            "Input sentence: <start> Cheers ! <end>\n",
            "Decoded sentence: За ваше здоровье!\n",
            "\n",
            "-\n",
            "Input sentence: <start> Eat it . <end>\n",
            "Decoded sentence: Съешь это.\n",
            "\n",
            "-\n",
            "Input sentence: <start> Eat up . <end>\n",
            "Decoded sentence: Доедай.\n",
            "\n",
            "-\n",
            "Input sentence: <start> Freeze ! <end>\n",
            "Decoded sentence: Замри!\n",
            "\n",
            "-\n",
            "Input sentence: <start> Freeze ! <end>\n",
            "Decoded sentence: Замри!\n",
            "\n",
            "-\n",
            "Input sentence: <start> Freeze ! <end>\n",
            "Decoded sentence: Замри!\n",
            "\n",
            "-\n",
            "Input sentence: <start> Freeze ! <end>\n",
            "Decoded sentence: Замри!\n",
            "\n",
            "-\n",
            "Input sentence: <start> Get up . <end>\n",
            "Decoded sentence: Поднимайся.\n",
            "\n",
            "-\n",
            "Input sentence: <start> Get up . <end>\n",
            "Decoded sentence: Поднимайся.\n",
            "\n",
            "-\n",
            "Input sentence: <start> Get up . <end>\n",
            "Decoded sentence: Поднимайся.\n",
            "\n",
            "-\n",
            "Input sentence: <start> Go now . <end>\n",
            "Decoded sentence: Иди сейчас.\n",
            "\n",
            "-\n",
            "Input sentence: <start> Go now . <end>\n",
            "Decoded sentence: Иди сейчас.\n",
            "\n",
            "-\n",
            "Input sentence: <start> Go now . <end>\n",
            "Decoded sentence: Иди сейчас.\n",
            "\n",
            "-\n",
            "Input sentence: <start> Go now . <end>\n",
            "Decoded sentence: Иди сейчас.\n",
            "\n",
            "-\n",
            "Input sentence: <start> Go now . <end>\n",
            "Decoded sentence: Иди сейчас.\n",
            "\n",
            "-\n",
            "Input sentence: <start> Go now . <end>\n",
            "Decoded sentence: Иди сейчас.\n",
            "\n",
            "-\n",
            "Input sentence: <start> Go now . <end>\n",
            "Decoded sentence: Иди сейчас.\n",
            "\n",
            "-\n",
            "Input sentence: <start> Go now . <end>\n",
            "Decoded sentence: Иди сейчас.\n",
            "\n",
            "-\n",
            "Input sentence: <start> Go now . <end>\n",
            "Decoded sentence: Иди сейчас.\n",
            "\n",
            "-\n",
            "Input sentence: <start> Got it ! <end>\n",
            "Decoded sentence: Понял!\n",
            "\n",
            "-\n",
            "Input sentence: <start> Got it ? <end>\n",
            "Decoded sentence: Усекла?\n",
            "\n",
            "-\n",
            "Input sentence: <start> Got it ? <end>\n",
            "Decoded sentence: Усекла?\n",
            "\n",
            "-\n",
            "Input sentence: <start> Got it ? <end>\n",
            "Decoded sentence: Усекла?\n",
            "\n",
            "-\n",
            "Input sentence: <start> Got it ? <end>\n",
            "Decoded sentence: Усекла?\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQUGOfGhMEji",
        "colab_type": "text"
      },
      "source": [
        "После увеличения батча до 128, перевод стал намного осмысленней"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_H4WitEMuV6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epMewCenMq4p",
        "colab_type": "text"
      },
      "source": [
        "seq2seq с вниманием"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SglTWtfiBAMq",
        "colab_type": "text"
      },
      "source": [
        "Поднимемся на уровень слов, чтобы можно было что-то более адекватное посчитать за адекватное время"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GQ76-ey92VW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "\n",
        "data_path = 'rus.txt'\n",
        "num_samples = 10000\n",
        "\n",
        "# tf.enable_eager_execution()\n",
        "\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "    w = w.strip()\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w\n",
        "\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    input_text, target_text, _ = line.split('\\t')\n",
        "    target_text = '\\t' + target_text + '\\n'\n",
        "    input_texts.append(preprocess_sentence(input_text))\n",
        "    target_texts.append(preprocess_sentence(target_text))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfrRBJVv92Rx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(lang):\n",
        "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "    lang_tokenizer.fit_on_texts(lang)\n",
        "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "    return tensor, lang_tokenizer"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yXhzVEb92N_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_tensor, inp_lang_tokenizer = tokenize(input_texts)\n",
        "target_tensor, targ_lang_tokenizer = tokenize(target_texts)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vd5QwoT492K3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMoRYSft92Hb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "\n",
        "vocab_inp_size = len(inp_lang_tokenizer.word_index)+1\n",
        "vocab_tar_size = len(targ_lang_tokenizer.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HLyTLHs92Ef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = tf.keras.layers.GRU(self.enc_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True)\n",
        "\n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        output, state = self.lstm(x, initial_state = hidden)\n",
        "        return output, state\n",
        "\n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.enc_units))\n",
        "\n",
        "    \n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        query_with_time_axis = tf.expand_dims(query, 1)\n",
        "        score = self.V(tf.nn.tanh(\n",
        "            self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights\n",
        "    \n",
        "    \n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = tf.keras.layers.GRU(self.dec_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True)\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "        self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "    def call(self, x, hidden, enc_output):\n",
        "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "        x = self.embedding(x)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        output, state = self.lstm(x)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "        x = self.fc(output)\n",
        "        return x, state, attention_weights"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9f2MzjT92Bd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caqtfNBm91-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "    loss = 0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "        dec_hidden = enc_hidden\n",
        "        dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "        for t in range(1, targ.shape[1]):\n",
        "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "            loss += loss_function(targ[:, t], predictions)\n",
        "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "    \n",
        "    batch_loss = (loss / int(targ.shape[1]))\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "    return batch_loss"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-nuiP5p916m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        },
        "outputId": "601060d4-35d8-470c-f6c1-e485a14dbb2e"
      },
      "source": [
        "EPOCHS = 35\n",
        "for epoch in range(EPOCHS):\n",
        "    enc_hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(inp, targ, enc_hidden)\n",
        "        total_loss += batch_loss\n",
        "    \n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Loss 0.1099\n",
            "Epoch 2 Loss 0.0317\n",
            "Epoch 3 Loss 0.0255\n",
            "Epoch 4 Loss 0.0214\n",
            "Epoch 5 Loss 0.0197\n",
            "Epoch 6 Loss 0.0180\n",
            "Epoch 7 Loss 0.0170\n",
            "Epoch 8 Loss 0.0171\n",
            "Epoch 9 Loss 0.0168\n",
            "Epoch 10 Loss 0.0159\n",
            "Epoch 11 Loss 0.0154\n",
            "Epoch 12 Loss 0.0139\n",
            "Epoch 13 Loss 0.0137\n",
            "Epoch 14 Loss 0.0140\n",
            "Epoch 15 Loss 0.0174\n",
            "Epoch 16 Loss 0.0139\n",
            "Epoch 17 Loss 0.0123\n",
            "Epoch 18 Loss 0.0114\n",
            "Epoch 19 Loss 0.0124\n",
            "Epoch 20 Loss 0.0107\n",
            "Epoch 21 Loss 0.0103\n",
            "Epoch 22 Loss 0.0110\n",
            "Epoch 23 Loss 0.0108\n",
            "Epoch 24 Loss 0.0101\n",
            "Epoch 25 Loss 0.0105\n",
            "Epoch 26 Loss 0.0092\n",
            "Epoch 27 Loss 0.0102\n",
            "Epoch 28 Loss 0.0097\n",
            "Epoch 29 Loss 0.0084\n",
            "Epoch 30 Loss 0.0081\n",
            "Epoch 31 Loss 0.0090\n",
            "Epoch 32 Loss 0.0078\n",
            "Epoch 33 Loss 0.0076\n",
            "Epoch 34 Loss 0.0075\n",
            "Epoch 35 Loss 0.0073\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6EiNQjtBf9n",
        "colab_type": "text"
      },
      "source": [
        "Некоторые украденные функции для оценки"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACgGscvl9lYp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n",
        "\n",
        "def evaluate(sentence):\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "    inputs = [inp_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    result = ''\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                             dec_hidden,\n",
        "                                                             enc_out)\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += targ_lang_tokenizer.index_word[predicted_id] + ' '\n",
        "\n",
        "        if targ_lang_tokenizer.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence, attention_plot\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "    return result, sentence, attention_plot\n",
        "\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(attention, cmap='viridis')\n",
        "    fontdict = {'fontsize': 14}\n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    plt.show()\n",
        "    \n",
        "def translate(sentence):\n",
        "    result, sentence, attention_plot = evaluate(sentence)\n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted translation: {}'.format(result))\n",
        "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-PczGSIBZsb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        },
        "outputId": "3a6a7022-1fae-4770-8101-37baa52eda64"
      },
      "source": [
        "%pylab inline\n",
        "\n",
        "translate(u'good morning')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n",
            "Input: <start> good morning <end>\n",
            "Predicted translation: ! <end> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['f', 'char']\n",
            "`%matplotlib` prevents importing * from pylab and numpy\n",
            "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijkBNJBWBZog",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgcRkv3hBow4",
        "colab_type": "text"
      },
      "source": [
        "## Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsnrJKTTBZlH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \"\"\"Calculate the attention weights.\n",
        "  q, k, v must have matching leading dimensions.\n",
        "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "  The mask has different shapes depending on its type(padding or look ahead) \n",
        "  but it must be broadcastable for addition.\n",
        "  \n",
        "  Args:\n",
        "    q: query shape == (..., seq_len_q, depth)\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable \n",
        "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "    \n",
        "  Returns:\n",
        "    output, attention_weights\n",
        "    \"\"\"\n",
        "\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "  \n",
        "  # scale matmul_qk\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "  # add the mask to the scaled tensor.\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "  # add up to 1.\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "    return output, attention_weights"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiQuqzq5BZhm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention, \n",
        "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        return output, attention_weights\n",
        "    \n",
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "  ])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwlPoIoNBZd-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, training, mask):\n",
        "\n",
        "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        return out2\n",
        "    \n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    \n",
        "    def call(self, x, enc_output, training, \n",
        "           look_ahead_mask, padding_mask):\n",
        "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "        attn2, attn_weights_block2 = self.mha2(\n",
        "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2\n",
        "    \n",
        "    \n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
        "                                                self.d_model)\n",
        "\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
        "                           for _ in range(num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "    def call(self, x, training, mask):\n",
        "\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        # adding embedding and position encoding.\n",
        "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "        return x  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1AsvDSzBZaW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
        "                           for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, enc_output, training, \n",
        "           look_ahead_mask, padding_mask):\n",
        "\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                                 look_ahead_mask, padding_mask)\n",
        "\n",
        "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "\n",
        "        # x.shape == (batch_size, target_seq_len, d_model)\n",
        "        return x, attention_weights"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqtS-OORBZXM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
        "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
        "                               input_vocab_size, pe_input, rate)\n",
        "\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
        "                               target_vocab_size, pe_target, rate)\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "    \n",
        "    def call(self, inp, tar, training, enc_padding_mask, \n",
        "           look_ahead_mask, dec_padding_mask):\n",
        "\n",
        "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "\n",
        "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "        dec_output, attention_weights = self.decoder(\n",
        "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "\n",
        "        return final_output, attention_weights"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cid4Lyj5BZTr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "\n",
        "input_vocab_size = len(inp_lang_tokenizer.index_word) + 2\n",
        "target_vocab_size = len(targ_lang_tokenizer.index_word) + 2\n",
        "dropout_rate = 0.1"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Cr4F3ExBZQN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tF6KSZhHBZLM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
        "                                     epsilon=1e-9)\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "  \n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9xS5TRt9lWH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "  \n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "  \n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    \n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "    \n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlLjNtt09lTr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='train_accuracy')\n",
        "\n",
        "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
        "                          input_vocab_size, target_vocab_size, \n",
        "                          pe_input=input_vocab_size, \n",
        "                          pe_target=target_vocab_size,\n",
        "                          rate=dropout_rate)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P34_G0ixDZs4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_masks(inp, tar):\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "  \n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "  \n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "  \n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WH_bSVVnDZoj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
        "]\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "  \n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "  \n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = transformer(inp, tar_inp, \n",
        "                                 True, \n",
        "                                 enc_padding_mask, \n",
        "                                 combined_mask, \n",
        "                                 dec_padding_mask)\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "  \n",
        "    train_loss(loss)\n",
        "    train_accuracy(tar_real, predictions)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71ggi6fMDZjw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3f523214-2928-4c5d-b069-4c0d08cb2b59"
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "  \n",
        "  # add extra dimensions to add the padding\n",
        "  # to the attention logits.\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # (seq_len, seq_len)\n",
        "\n",
        "for epoch in range(100):\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "  \n",
        "    for (batch, (inp, tar)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        train_step(inp, tar)\n",
        "        if batch % 50 == 0:\n",
        "            print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
        "              epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
        "    \n",
        "    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
        "                                                train_loss.result(), \n",
        "                                                train_accuracy.result()))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 2.4005 Accuracy 0.0742\n",
            "Epoch 1 Batch 50 Loss 1.6452 Accuracy 0.1802\n",
            "Epoch 1 Batch 100 Loss 1.3375 Accuracy 0.2269\n",
            "Epoch 1 Loss 1.2143 Accuracy 0.2590\n",
            "Epoch 2 Batch 0 Loss 0.4798 Accuracy 0.4375\n",
            "Epoch 2 Batch 50 Loss 0.2827 Accuracy 0.4681\n",
            "Epoch 2 Batch 100 Loss 0.2231 Accuracy 0.4771\n",
            "Epoch 2 Loss 0.2066 Accuracy 0.4796\n",
            "Epoch 3 Batch 0 Loss 0.1212 Accuracy 0.4805\n",
            "Epoch 3 Batch 50 Loss 0.1234 Accuracy 0.4916\n",
            "Epoch 3 Batch 100 Loss 0.1187 Accuracy 0.4915\n",
            "Epoch 3 Loss 0.1190 Accuracy 0.4916\n",
            "Epoch 4 Batch 0 Loss 0.1878 Accuracy 0.4922\n",
            "Epoch 4 Batch 50 Loss 0.1184 Accuracy 0.4920\n",
            "Epoch 4 Batch 100 Loss 0.1043 Accuracy 0.4923\n",
            "Epoch 4 Loss 0.1033 Accuracy 0.4921\n",
            "Epoch 5 Batch 0 Loss 0.1418 Accuracy 0.4883\n",
            "Epoch 5 Batch 50 Loss 0.0891 Accuracy 0.4923\n",
            "Epoch 5 Batch 100 Loss 0.0916 Accuracy 0.4923\n",
            "Epoch 5 Loss 0.0894 Accuracy 0.4924\n",
            "Epoch 6 Batch 0 Loss 0.0439 Accuracy 0.4922\n",
            "Epoch 6 Batch 50 Loss 0.0788 Accuracy 0.4944\n",
            "Epoch 6 Batch 100 Loss 0.0759 Accuracy 0.4943\n",
            "Epoch 6 Loss 0.0777 Accuracy 0.4943\n",
            "Epoch 7 Batch 0 Loss 0.0389 Accuracy 0.4961\n",
            "Epoch 7 Batch 50 Loss 0.0718 Accuracy 0.4950\n",
            "Epoch 7 Batch 100 Loss 0.0782 Accuracy 0.4934\n",
            "Epoch 7 Loss 0.0774 Accuracy 0.4939\n",
            "Epoch 8 Batch 0 Loss 0.0465 Accuracy 0.4922\n",
            "Epoch 8 Batch 50 Loss 0.0636 Accuracy 0.4953\n",
            "Epoch 8 Batch 100 Loss 0.0651 Accuracy 0.4954\n",
            "Epoch 8 Loss 0.0672 Accuracy 0.4952\n",
            "Epoch 9 Batch 0 Loss 0.0882 Accuracy 0.4961\n",
            "Epoch 9 Batch 50 Loss 0.0578 Accuracy 0.4958\n",
            "Epoch 9 Batch 100 Loss 0.0602 Accuracy 0.4955\n",
            "Epoch 9 Loss 0.0615 Accuracy 0.4956\n",
            "Epoch 10 Batch 0 Loss 0.1644 Accuracy 0.4883\n",
            "Epoch 10 Batch 50 Loss 0.0559 Accuracy 0.4956\n",
            "Epoch 10 Batch 100 Loss 0.0573 Accuracy 0.4962\n",
            "Epoch 10 Loss 0.0570 Accuracy 0.4961\n",
            "Epoch 11 Batch 0 Loss 0.0506 Accuracy 0.4922\n",
            "Epoch 11 Batch 50 Loss 0.0573 Accuracy 0.4958\n",
            "Epoch 11 Batch 100 Loss 0.0560 Accuracy 0.4966\n",
            "Epoch 11 Loss 0.0579 Accuracy 0.4963\n",
            "Epoch 12 Batch 0 Loss 0.0269 Accuracy 0.4961\n",
            "Epoch 12 Batch 50 Loss 0.0552 Accuracy 0.4974\n",
            "Epoch 12 Batch 100 Loss 0.0547 Accuracy 0.4967\n",
            "Epoch 12 Loss 0.0553 Accuracy 0.4967\n",
            "Epoch 13 Batch 0 Loss 0.1320 Accuracy 0.4922\n",
            "Epoch 13 Batch 50 Loss 0.0483 Accuracy 0.4964\n",
            "Epoch 13 Batch 100 Loss 0.0563 Accuracy 0.4962\n",
            "Epoch 13 Loss 0.0554 Accuracy 0.4964\n",
            "Epoch 14 Batch 0 Loss 0.0377 Accuracy 0.4961\n",
            "Epoch 14 Batch 50 Loss 0.0516 Accuracy 0.4971\n",
            "Epoch 14 Batch 100 Loss 0.0533 Accuracy 0.4971\n",
            "Epoch 14 Loss 0.0519 Accuracy 0.4972\n",
            "Epoch 15 Batch 0 Loss 0.0039 Accuracy 0.5039\n",
            "Epoch 15 Batch 50 Loss 0.0540 Accuracy 0.4955\n",
            "Epoch 15 Batch 100 Loss 0.0574 Accuracy 0.4949\n",
            "Epoch 15 Loss 0.0574 Accuracy 0.4958\n",
            "Epoch 16 Batch 0 Loss 0.0261 Accuracy 0.4961\n",
            "Epoch 16 Batch 50 Loss 0.0520 Accuracy 0.4969\n",
            "Epoch 16 Batch 100 Loss 0.0509 Accuracy 0.4970\n",
            "Epoch 16 Loss 0.0520 Accuracy 0.4970\n",
            "Epoch 17 Batch 0 Loss 0.0295 Accuracy 0.5039\n",
            "Epoch 17 Batch 50 Loss 0.0495 Accuracy 0.4982\n",
            "Epoch 17 Batch 100 Loss 0.0483 Accuracy 0.4978\n",
            "Epoch 17 Loss 0.0496 Accuracy 0.4974\n",
            "Epoch 18 Batch 0 Loss 0.0069 Accuracy 0.5000\n",
            "Epoch 18 Batch 50 Loss 0.0470 Accuracy 0.4976\n",
            "Epoch 18 Batch 100 Loss 0.0501 Accuracy 0.4972\n",
            "Epoch 18 Loss 0.0506 Accuracy 0.4972\n",
            "Epoch 19 Batch 0 Loss 0.0306 Accuracy 0.5000\n",
            "Epoch 19 Batch 50 Loss 0.0441 Accuracy 0.4979\n",
            "Epoch 19 Batch 100 Loss 0.0455 Accuracy 0.4981\n",
            "Epoch 19 Loss 0.0499 Accuracy 0.4978\n",
            "Epoch 20 Batch 0 Loss 0.0314 Accuracy 0.4961\n",
            "Epoch 20 Batch 50 Loss 0.0445 Accuracy 0.4979\n",
            "Epoch 20 Batch 100 Loss 0.0479 Accuracy 0.4971\n",
            "Epoch 20 Loss 0.0477 Accuracy 0.4976\n",
            "Epoch 21 Batch 0 Loss 0.0210 Accuracy 0.5078\n",
            "Epoch 21 Batch 50 Loss 0.0447 Accuracy 0.4990\n",
            "Epoch 21 Batch 100 Loss 0.0492 Accuracy 0.4981\n",
            "Epoch 21 Loss 0.0515 Accuracy 0.4979\n",
            "Epoch 22 Batch 0 Loss 0.0655 Accuracy 0.4883\n",
            "Epoch 22 Batch 50 Loss 0.0454 Accuracy 0.4979\n",
            "Epoch 22 Batch 100 Loss 0.0502 Accuracy 0.4978\n",
            "Epoch 22 Loss 0.0508 Accuracy 0.4974\n",
            "Epoch 23 Batch 0 Loss 0.0602 Accuracy 0.5000\n",
            "Epoch 23 Batch 50 Loss 0.0479 Accuracy 0.4999\n",
            "Epoch 23 Batch 100 Loss 0.0503 Accuracy 0.4983\n",
            "Epoch 23 Loss 0.0510 Accuracy 0.4978\n",
            "Epoch 24 Batch 0 Loss 0.1485 Accuracy 0.5000\n",
            "Epoch 24 Batch 50 Loss 0.0501 Accuracy 0.4978\n",
            "Epoch 24 Batch 100 Loss 0.0540 Accuracy 0.4958\n",
            "Epoch 24 Loss 0.0532 Accuracy 0.4961\n",
            "Epoch 25 Batch 0 Loss 0.0221 Accuracy 0.4961\n",
            "Epoch 25 Batch 50 Loss 0.0604 Accuracy 0.4966\n",
            "Epoch 25 Batch 100 Loss 0.0556 Accuracy 0.4968\n",
            "Epoch 25 Loss 0.0538 Accuracy 0.4970\n",
            "Epoch 26 Batch 0 Loss 0.0103 Accuracy 0.5039\n",
            "Epoch 26 Batch 50 Loss 0.0543 Accuracy 0.4976\n",
            "Epoch 26 Batch 100 Loss 0.0553 Accuracy 0.4977\n",
            "Epoch 26 Loss 0.0533 Accuracy 0.4978\n",
            "Epoch 27 Batch 0 Loss 0.0340 Accuracy 0.5000\n",
            "Epoch 27 Batch 50 Loss 0.0505 Accuracy 0.4966\n",
            "Epoch 27 Batch 100 Loss 0.0541 Accuracy 0.4966\n",
            "Epoch 27 Loss 0.0555 Accuracy 0.4969\n",
            "Epoch 28 Batch 0 Loss 0.0145 Accuracy 0.5000\n",
            "Epoch 28 Batch 50 Loss 0.0549 Accuracy 0.4981\n",
            "Epoch 28 Batch 100 Loss 0.0527 Accuracy 0.4977\n",
            "Epoch 28 Loss 0.0566 Accuracy 0.4970\n",
            "Epoch 29 Batch 0 Loss 0.2223 Accuracy 0.4844\n",
            "Epoch 29 Batch 50 Loss 0.0672 Accuracy 0.4960\n",
            "Epoch 29 Batch 100 Loss 0.0583 Accuracy 0.4974\n",
            "Epoch 29 Loss 0.0554 Accuracy 0.4977\n",
            "Epoch 30 Batch 0 Loss 0.0947 Accuracy 0.4961\n",
            "Epoch 30 Batch 50 Loss 0.0548 Accuracy 0.4974\n",
            "Epoch 30 Batch 100 Loss 0.0514 Accuracy 0.4981\n",
            "Epoch 30 Loss 0.0514 Accuracy 0.4979\n",
            "Epoch 31 Batch 0 Loss 0.0950 Accuracy 0.4961\n",
            "Epoch 31 Batch 50 Loss 0.1031 Accuracy 0.4926\n",
            "Epoch 31 Batch 100 Loss 0.0817 Accuracy 0.4944\n",
            "Epoch 31 Loss 0.0764 Accuracy 0.4952\n",
            "Epoch 32 Batch 0 Loss 0.0175 Accuracy 0.5000\n",
            "Epoch 32 Batch 50 Loss 0.0494 Accuracy 0.4983\n",
            "Epoch 32 Batch 100 Loss 0.0537 Accuracy 0.4979\n",
            "Epoch 32 Loss 0.0568 Accuracy 0.4979\n",
            "Epoch 33 Batch 0 Loss 0.0302 Accuracy 0.5000\n",
            "Epoch 33 Batch 50 Loss 0.0544 Accuracy 0.4982\n",
            "Epoch 33 Batch 100 Loss 0.0591 Accuracy 0.4967\n",
            "Epoch 33 Loss 0.0625 Accuracy 0.4962\n",
            "Epoch 34 Batch 0 Loss 0.0114 Accuracy 0.5000\n",
            "Epoch 34 Batch 50 Loss 0.0729 Accuracy 0.4965\n",
            "Epoch 34 Batch 100 Loss 0.0632 Accuracy 0.4970\n",
            "Epoch 34 Loss 0.0589 Accuracy 0.4975\n",
            "Epoch 35 Batch 0 Loss 0.0647 Accuracy 0.4961\n",
            "Epoch 35 Batch 50 Loss 0.0470 Accuracy 0.4975\n",
            "Epoch 35 Batch 100 Loss 0.0505 Accuracy 0.4981\n",
            "Epoch 35 Loss 0.1431 Accuracy 0.4856\n",
            "Epoch 36 Batch 0 Loss 0.7393 Accuracy 0.2773\n",
            "Epoch 36 Batch 50 Loss 0.7925 Accuracy 0.2695\n",
            "Epoch 36 Batch 100 Loss 0.6330 Accuracy 0.3241\n",
            "Epoch 36 Loss 0.5259 Accuracy 0.3567\n",
            "Epoch 37 Batch 0 Loss 0.0063 Accuracy 0.5000\n",
            "Epoch 37 Batch 50 Loss 0.0796 Accuracy 0.4939\n",
            "Epoch 37 Batch 100 Loss 0.0770 Accuracy 0.4940\n",
            "Epoch 37 Loss 0.0712 Accuracy 0.4943\n",
            "Epoch 38 Batch 0 Loss 0.0102 Accuracy 0.5000\n",
            "Epoch 38 Batch 50 Loss 0.0532 Accuracy 0.4953\n",
            "Epoch 38 Batch 100 Loss 0.0570 Accuracy 0.4948\n",
            "Epoch 38 Loss 0.0551 Accuracy 0.4951\n",
            "Epoch 39 Batch 0 Loss 0.0369 Accuracy 0.4922\n",
            "Epoch 39 Batch 50 Loss 0.0447 Accuracy 0.4966\n",
            "Epoch 39 Batch 100 Loss 0.0567 Accuracy 0.4949\n",
            "Epoch 39 Loss 0.0572 Accuracy 0.4950\n",
            "Epoch 40 Batch 0 Loss 0.0450 Accuracy 0.5000\n",
            "Epoch 40 Batch 50 Loss 0.0556 Accuracy 0.4965\n",
            "Epoch 40 Batch 100 Loss 0.0564 Accuracy 0.4959\n",
            "Epoch 40 Loss 0.0557 Accuracy 0.4957\n",
            "Epoch 41 Batch 0 Loss 0.0396 Accuracy 0.4961\n",
            "Epoch 41 Batch 50 Loss 0.0563 Accuracy 0.4947\n",
            "Epoch 41 Batch 100 Loss 0.0565 Accuracy 0.4951\n",
            "Epoch 41 Loss 0.0581 Accuracy 0.4952\n",
            "Epoch 42 Batch 0 Loss 0.0611 Accuracy 0.4961\n",
            "Epoch 42 Batch 50 Loss 0.0587 Accuracy 0.4927\n",
            "Epoch 42 Batch 100 Loss 0.0582 Accuracy 0.4937\n",
            "Epoch 42 Loss 0.0590 Accuracy 0.4938\n",
            "Epoch 43 Batch 0 Loss 0.0546 Accuracy 0.4922\n",
            "Epoch 43 Batch 50 Loss 0.0521 Accuracy 0.4957\n",
            "Epoch 43 Batch 100 Loss 0.0495 Accuracy 0.4958\n",
            "Epoch 43 Loss 0.0503 Accuracy 0.4956\n",
            "Epoch 44 Batch 0 Loss 0.0134 Accuracy 0.5000\n",
            "Epoch 44 Batch 50 Loss 0.0484 Accuracy 0.4959\n",
            "Epoch 44 Batch 100 Loss 0.0509 Accuracy 0.4952\n",
            "Epoch 44 Loss 0.0627 Accuracy 0.4944\n",
            "Epoch 45 Batch 0 Loss 0.0606 Accuracy 0.4961\n",
            "Epoch 45 Batch 50 Loss 0.0583 Accuracy 0.4954\n",
            "Epoch 45 Batch 100 Loss 0.0536 Accuracy 0.4957\n",
            "Epoch 45 Loss 0.0535 Accuracy 0.4955\n",
            "Epoch 46 Batch 0 Loss 0.0303 Accuracy 0.4961\n",
            "Epoch 46 Batch 50 Loss 0.0470 Accuracy 0.4960\n",
            "Epoch 46 Batch 100 Loss 0.0453 Accuracy 0.4963\n",
            "Epoch 46 Loss 0.0492 Accuracy 0.4958\n",
            "Epoch 47 Batch 0 Loss 0.0396 Accuracy 0.4922\n",
            "Epoch 47 Batch 50 Loss 0.0551 Accuracy 0.4952\n",
            "Epoch 47 Batch 100 Loss 0.0498 Accuracy 0.4954\n",
            "Epoch 47 Loss 0.0513 Accuracy 0.4955\n",
            "Epoch 48 Batch 0 Loss 0.0521 Accuracy 0.4922\n",
            "Epoch 48 Batch 50 Loss 0.0503 Accuracy 0.4956\n",
            "Epoch 48 Batch 100 Loss 0.0505 Accuracy 0.4956\n",
            "Epoch 48 Loss 0.0514 Accuracy 0.4956\n",
            "Epoch 49 Batch 0 Loss 0.0940 Accuracy 0.5000\n",
            "Epoch 49 Batch 50 Loss 0.0554 Accuracy 0.4953\n",
            "Epoch 49 Batch 100 Loss 0.0499 Accuracy 0.4953\n",
            "Epoch 49 Loss 0.0490 Accuracy 0.4956\n",
            "Epoch 50 Batch 0 Loss 0.0939 Accuracy 0.5000\n",
            "Epoch 50 Batch 50 Loss 0.0502 Accuracy 0.4967\n",
            "Epoch 50 Batch 100 Loss 0.0479 Accuracy 0.4964\n",
            "Epoch 50 Loss 0.0471 Accuracy 0.4965\n",
            "Epoch 51 Batch 0 Loss 0.0430 Accuracy 0.4883\n",
            "Epoch 51 Batch 50 Loss 0.0473 Accuracy 0.4955\n",
            "Epoch 51 Batch 100 Loss 0.0958 Accuracy 0.4892\n",
            "Epoch 51 Loss 0.1083 Accuracy 0.4858\n",
            "Epoch 52 Batch 0 Loss 0.0878 Accuracy 0.4883\n",
            "Epoch 52 Batch 50 Loss 0.0758 Accuracy 0.4938\n",
            "Epoch 52 Batch 100 Loss 0.0641 Accuracy 0.4947\n",
            "Epoch 52 Loss 0.0624 Accuracy 0.4948\n",
            "Epoch 53 Batch 0 Loss 0.0276 Accuracy 0.4961\n",
            "Epoch 53 Batch 50 Loss 0.0578 Accuracy 0.4946\n",
            "Epoch 53 Batch 100 Loss 0.3526 Accuracy 0.3961\n",
            "Epoch 53 Loss 0.4106 Accuracy 0.3745\n",
            "Epoch 54 Batch 0 Loss 0.6289 Accuracy 0.2734\n",
            "Epoch 54 Batch 50 Loss 0.6469 Accuracy 0.2835\n",
            "Epoch 54 Batch 100 Loss 0.6398 Accuracy 0.2848\n",
            "Epoch 54 Loss 0.6333 Accuracy 0.2908\n",
            "Epoch 55 Batch 0 Loss 0.5721 Accuracy 0.3164\n",
            "Epoch 55 Batch 50 Loss 0.3789 Accuracy 0.4269\n",
            "Epoch 55 Batch 100 Loss 0.2751 Accuracy 0.4506\n",
            "Epoch 55 Loss 0.2504 Accuracy 0.4565\n",
            "Epoch 56 Batch 0 Loss 0.1393 Accuracy 0.4805\n",
            "Epoch 56 Batch 50 Loss 0.1447 Accuracy 0.4825\n",
            "Epoch 56 Batch 100 Loss 0.1210 Accuracy 0.4863\n",
            "Epoch 56 Loss 0.1154 Accuracy 0.4873\n",
            "Epoch 57 Batch 0 Loss 0.0984 Accuracy 0.4844\n",
            "Epoch 57 Batch 50 Loss 0.0929 Accuracy 0.4897\n",
            "Epoch 57 Batch 100 Loss 0.1006 Accuracy 0.4877\n",
            "Epoch 57 Loss 0.1009 Accuracy 0.4885\n",
            "Epoch 58 Batch 0 Loss 0.0734 Accuracy 0.5000\n",
            "Epoch 58 Batch 50 Loss 0.1105 Accuracy 0.4872\n",
            "Epoch 58 Batch 100 Loss 0.1003 Accuracy 0.4887\n",
            "Epoch 58 Loss 0.1014 Accuracy 0.4889\n",
            "Epoch 59 Batch 0 Loss 0.0187 Accuracy 0.5000\n",
            "Epoch 59 Batch 50 Loss 0.0817 Accuracy 0.4906\n",
            "Epoch 59 Batch 100 Loss 0.0974 Accuracy 0.4895\n",
            "Epoch 59 Loss 0.0950 Accuracy 0.4899\n",
            "Epoch 60 Batch 0 Loss 0.0698 Accuracy 0.4844\n",
            "Epoch 60 Batch 50 Loss 0.0798 Accuracy 0.4920\n",
            "Epoch 60 Batch 100 Loss 0.0916 Accuracy 0.4905\n",
            "Epoch 60 Loss 0.0896 Accuracy 0.4907\n",
            "Epoch 61 Batch 0 Loss 0.0483 Accuracy 0.4961\n",
            "Epoch 61 Batch 50 Loss 0.0776 Accuracy 0.4937\n",
            "Epoch 61 Batch 100 Loss 0.0840 Accuracy 0.4936\n",
            "Epoch 61 Loss 0.0824 Accuracy 0.4930\n",
            "Epoch 62 Batch 0 Loss 0.0368 Accuracy 0.4922\n",
            "Epoch 62 Batch 50 Loss 0.0719 Accuracy 0.4938\n",
            "Epoch 62 Batch 100 Loss 0.0891 Accuracy 0.4918\n",
            "Epoch 62 Loss 0.0860 Accuracy 0.4919\n",
            "Epoch 63 Batch 0 Loss 0.1376 Accuracy 0.4883\n",
            "Epoch 63 Batch 50 Loss 0.0675 Accuracy 0.4932\n",
            "Epoch 63 Batch 100 Loss 0.1099 Accuracy 0.4878\n",
            "Epoch 63 Loss 0.1060 Accuracy 0.4881\n",
            "Epoch 64 Batch 0 Loss 0.0935 Accuracy 0.5000\n",
            "Epoch 64 Batch 50 Loss 0.0717 Accuracy 0.4926\n",
            "Epoch 64 Batch 100 Loss 0.0758 Accuracy 0.4934\n",
            "Epoch 64 Loss 0.0720 Accuracy 0.4937\n",
            "Epoch 65 Batch 0 Loss 0.0314 Accuracy 0.4961\n",
            "Epoch 65 Batch 50 Loss 0.1044 Accuracy 0.4894\n",
            "Epoch 65 Batch 100 Loss 0.0934 Accuracy 0.4908\n",
            "Epoch 65 Loss 0.0893 Accuracy 0.4913\n",
            "Epoch 66 Batch 0 Loss 0.1449 Accuracy 0.4922\n",
            "Epoch 66 Batch 50 Loss 0.1210 Accuracy 0.4864\n",
            "Epoch 66 Batch 100 Loss 0.0957 Accuracy 0.4900\n",
            "Epoch 66 Loss 0.0926 Accuracy 0.4909\n",
            "Epoch 67 Batch 0 Loss 0.0189 Accuracy 0.5000\n",
            "Epoch 67 Batch 50 Loss 0.0697 Accuracy 0.4944\n",
            "Epoch 67 Batch 100 Loss 0.0661 Accuracy 0.4936\n",
            "Epoch 67 Loss 0.0691 Accuracy 0.4935\n",
            "Epoch 68 Batch 0 Loss 0.0673 Accuracy 0.4922\n",
            "Epoch 68 Batch 50 Loss 0.0674 Accuracy 0.4933\n",
            "Epoch 68 Batch 100 Loss 0.0723 Accuracy 0.4931\n",
            "Epoch 68 Loss 0.0871 Accuracy 0.4917\n",
            "Epoch 69 Batch 0 Loss 0.0772 Accuracy 0.4961\n",
            "Epoch 69 Batch 50 Loss 0.0603 Accuracy 0.4955\n",
            "Epoch 69 Batch 100 Loss 0.0672 Accuracy 0.4940\n",
            "Epoch 69 Loss 0.0710 Accuracy 0.4937\n",
            "Epoch 70 Batch 0 Loss 0.0348 Accuracy 0.5000\n",
            "Epoch 70 Batch 50 Loss 0.0634 Accuracy 0.4952\n",
            "Epoch 70 Batch 100 Loss 0.0625 Accuracy 0.4948\n",
            "Epoch 70 Loss 0.0711 Accuracy 0.4934\n",
            "Epoch 71 Batch 0 Loss 0.0766 Accuracy 0.4883\n",
            "Epoch 71 Batch 50 Loss 0.0838 Accuracy 0.4924\n",
            "Epoch 71 Batch 100 Loss 0.0706 Accuracy 0.4935\n",
            "Epoch 71 Loss 0.0710 Accuracy 0.4937\n",
            "Epoch 72 Batch 0 Loss 0.0941 Accuracy 0.4805\n",
            "Epoch 72 Batch 50 Loss 0.0726 Accuracy 0.4942\n",
            "Epoch 72 Batch 100 Loss 0.0667 Accuracy 0.4945\n",
            "Epoch 72 Loss 0.0626 Accuracy 0.4948\n",
            "Epoch 73 Batch 0 Loss 0.0138 Accuracy 0.4961\n",
            "Epoch 73 Batch 50 Loss 0.0525 Accuracy 0.4959\n",
            "Epoch 73 Batch 100 Loss 0.0684 Accuracy 0.4947\n",
            "Epoch 73 Loss 0.0641 Accuracy 0.4950\n",
            "Epoch 74 Batch 0 Loss 0.0984 Accuracy 0.4961\n",
            "Epoch 74 Batch 50 Loss 0.0644 Accuracy 0.4949\n",
            "Epoch 74 Batch 100 Loss 0.0641 Accuracy 0.4942\n",
            "Epoch 74 Loss 0.0639 Accuracy 0.4944\n",
            "Epoch 75 Batch 0 Loss 0.0610 Accuracy 0.4961\n",
            "Epoch 75 Batch 50 Loss 0.0707 Accuracy 0.4941\n",
            "Epoch 75 Batch 100 Loss 0.0706 Accuracy 0.4935\n",
            "Epoch 75 Loss 0.0738 Accuracy 0.4929\n",
            "Epoch 76 Batch 0 Loss 0.0121 Accuracy 0.5000\n",
            "Epoch 76 Batch 50 Loss 0.0636 Accuracy 0.4958\n",
            "Epoch 76 Batch 100 Loss 0.0660 Accuracy 0.4945\n",
            "Epoch 76 Loss 0.0695 Accuracy 0.4939\n",
            "Epoch 77 Batch 0 Loss 0.1755 Accuracy 0.4805\n",
            "Epoch 77 Batch 50 Loss 0.0732 Accuracy 0.4926\n",
            "Epoch 77 Batch 100 Loss 0.0722 Accuracy 0.4935\n",
            "Epoch 77 Loss 0.0689 Accuracy 0.4937\n",
            "Epoch 78 Batch 0 Loss 0.0421 Accuracy 0.5000\n",
            "Epoch 78 Batch 50 Loss 0.0662 Accuracy 0.4947\n",
            "Epoch 78 Batch 100 Loss 0.0658 Accuracy 0.4949\n",
            "Epoch 78 Loss 0.0672 Accuracy 0.4947\n",
            "Epoch 79 Batch 0 Loss 0.0289 Accuracy 0.5039\n",
            "Epoch 79 Batch 50 Loss 0.0696 Accuracy 0.4936\n",
            "Epoch 79 Batch 100 Loss 0.0655 Accuracy 0.4942\n",
            "Epoch 79 Loss 0.0669 Accuracy 0.4946\n",
            "Epoch 80 Batch 0 Loss 0.0264 Accuracy 0.4922\n",
            "Epoch 80 Batch 50 Loss 0.0777 Accuracy 0.4920\n",
            "Epoch 80 Batch 100 Loss 0.0751 Accuracy 0.4930\n",
            "Epoch 80 Loss 0.0703 Accuracy 0.4937\n",
            "Epoch 81 Batch 0 Loss 0.0331 Accuracy 0.5000\n",
            "Epoch 81 Batch 50 Loss 0.0578 Accuracy 0.4959\n",
            "Epoch 81 Batch 100 Loss 0.0596 Accuracy 0.4952\n",
            "Epoch 81 Loss 0.0587 Accuracy 0.4951\n",
            "Epoch 82 Batch 0 Loss 0.0424 Accuracy 0.4883\n",
            "Epoch 82 Batch 50 Loss 0.0573 Accuracy 0.4946\n",
            "Epoch 82 Batch 100 Loss 0.0633 Accuracy 0.4942\n",
            "Epoch 82 Loss 0.0661 Accuracy 0.4940\n",
            "Epoch 83 Batch 0 Loss 0.0367 Accuracy 0.4961\n",
            "Epoch 83 Batch 50 Loss 0.0563 Accuracy 0.4954\n",
            "Epoch 83 Batch 100 Loss 0.0599 Accuracy 0.4952\n",
            "Epoch 83 Loss 0.0584 Accuracy 0.4948\n",
            "Epoch 84 Batch 0 Loss 0.0079 Accuracy 0.5000\n",
            "Epoch 84 Batch 50 Loss 0.0669 Accuracy 0.4927\n",
            "Epoch 84 Batch 100 Loss 0.0581 Accuracy 0.4938\n",
            "Epoch 84 Loss 0.0590 Accuracy 0.4938\n",
            "Epoch 85 Batch 0 Loss 0.0708 Accuracy 0.4805\n",
            "Epoch 85 Batch 50 Loss 0.0656 Accuracy 0.4932\n",
            "Epoch 85 Batch 100 Loss 0.0623 Accuracy 0.4941\n",
            "Epoch 85 Loss 0.0657 Accuracy 0.4941\n",
            "Epoch 86 Batch 0 Loss 0.0694 Accuracy 0.4922\n",
            "Epoch 86 Batch 50 Loss 0.0637 Accuracy 0.4947\n",
            "Epoch 86 Batch 100 Loss 0.0578 Accuracy 0.4948\n",
            "Epoch 86 Loss 0.0599 Accuracy 0.4947\n",
            "Epoch 87 Batch 0 Loss 0.0363 Accuracy 0.4922\n",
            "Epoch 87 Batch 50 Loss 0.0582 Accuracy 0.4954\n",
            "Epoch 87 Batch 100 Loss 0.0542 Accuracy 0.4956\n",
            "Epoch 87 Loss 0.0584 Accuracy 0.4954\n",
            "Epoch 88 Batch 0 Loss 0.0057 Accuracy 0.5000\n",
            "Epoch 88 Batch 50 Loss 0.0650 Accuracy 0.4940\n",
            "Epoch 88 Batch 100 Loss 0.0598 Accuracy 0.4945\n",
            "Epoch 88 Loss 0.0659 Accuracy 0.4935\n",
            "Epoch 89 Batch 0 Loss 0.0343 Accuracy 0.4922\n",
            "Epoch 89 Batch 50 Loss 0.0477 Accuracy 0.4957\n",
            "Epoch 89 Batch 100 Loss 0.0595 Accuracy 0.4952\n",
            "Epoch 89 Loss 0.0589 Accuracy 0.4951\n",
            "Epoch 90 Batch 0 Loss 0.0929 Accuracy 0.4844\n",
            "Epoch 90 Batch 50 Loss 0.0555 Accuracy 0.4950\n",
            "Epoch 90 Batch 100 Loss 0.0605 Accuracy 0.4949\n",
            "Epoch 90 Loss 0.0605 Accuracy 0.4947\n",
            "Epoch 91 Batch 0 Loss 0.0107 Accuracy 0.5000\n",
            "Epoch 91 Batch 50 Loss 0.0524 Accuracy 0.4953\n",
            "Epoch 91 Batch 100 Loss 0.0653 Accuracy 0.4938\n",
            "Epoch 91 Loss 0.0638 Accuracy 0.4942\n",
            "Epoch 92 Batch 0 Loss 0.0335 Accuracy 0.4922\n",
            "Epoch 92 Batch 50 Loss 0.0553 Accuracy 0.4946\n",
            "Epoch 92 Batch 100 Loss 0.0552 Accuracy 0.4943\n",
            "Epoch 92 Loss 0.0569 Accuracy 0.4943\n",
            "Epoch 93 Batch 0 Loss 0.0619 Accuracy 0.4844\n",
            "Epoch 93 Batch 50 Loss 0.0686 Accuracy 0.4938\n",
            "Epoch 93 Batch 100 Loss 0.0587 Accuracy 0.4943\n",
            "Epoch 93 Loss 0.0586 Accuracy 0.4946\n",
            "Epoch 94 Batch 0 Loss 0.0429 Accuracy 0.5117\n",
            "Epoch 94 Batch 50 Loss 0.0737 Accuracy 0.4952\n",
            "Epoch 94 Batch 100 Loss 0.0617 Accuracy 0.4955\n",
            "Epoch 94 Loss 0.0603 Accuracy 0.4953\n",
            "Epoch 95 Batch 0 Loss 0.0937 Accuracy 0.4961\n",
            "Epoch 95 Batch 50 Loss 0.0591 Accuracy 0.4943\n",
            "Epoch 95 Batch 100 Loss 0.0632 Accuracy 0.4933\n",
            "Epoch 95 Loss 0.0592 Accuracy 0.4939\n",
            "Epoch 96 Batch 0 Loss 0.0157 Accuracy 0.4922\n",
            "Epoch 96 Batch 50 Loss 0.0494 Accuracy 0.4966\n",
            "Epoch 96 Batch 100 Loss 0.0553 Accuracy 0.4952\n",
            "Epoch 96 Loss 0.0582 Accuracy 0.4950\n",
            "Epoch 97 Batch 0 Loss 0.1892 Accuracy 0.4883\n",
            "Epoch 97 Batch 50 Loss 0.0420 Accuracy 0.4959\n",
            "Epoch 97 Batch 100 Loss 0.0512 Accuracy 0.4953\n",
            "Epoch 97 Loss 0.0538 Accuracy 0.4951\n",
            "Epoch 98 Batch 0 Loss 0.1123 Accuracy 0.4961\n",
            "Epoch 98 Batch 50 Loss 0.0529 Accuracy 0.4959\n",
            "Epoch 98 Batch 100 Loss 0.0552 Accuracy 0.4952\n",
            "Epoch 98 Loss 0.0548 Accuracy 0.4952\n",
            "Epoch 99 Batch 0 Loss 0.0723 Accuracy 0.4922\n",
            "Epoch 99 Batch 50 Loss 0.0551 Accuracy 0.4956\n",
            "Epoch 99 Batch 100 Loss 0.0647 Accuracy 0.4950\n",
            "Epoch 99 Loss 0.0614 Accuracy 0.4948\n",
            "Epoch 100 Batch 0 Loss 0.0561 Accuracy 0.4922\n",
            "Epoch 100 Batch 50 Loss 0.0483 Accuracy 0.4955\n",
            "Epoch 100 Batch 100 Loss 0.0471 Accuracy 0.4958\n",
            "Epoch 100 Loss 0.0516 Accuracy 0.4955\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jst1F64eDZWS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "1c478d2d-6cef-47fa-e559-3911f4e2f964"
      },
      "source": [
        "def evaluate(inp_sentence):\n",
        "    start_token = [1]\n",
        "    end_token = [2]\n",
        "  \n",
        "    sentence = preprocess_sentence(inp_sentence)\n",
        "    inputs = [inp_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
        "    \n",
        "    encoder_input = tf.expand_dims(inputs, 0)\n",
        "  \n",
        "  # as the target is english, the first word to the transformer should be the\n",
        "  # english start token.\n",
        "    decoder_input = [1]\n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "    \n",
        "    for i in range(max_length_targ):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "            encoder_input, output)\n",
        "  \n",
        "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "        predictions, attention_weights = transformer(encoder_input, \n",
        "                                                 output,\n",
        "                                                 False,\n",
        "                                                 enc_padding_mask,\n",
        "                                                 combined_mask,\n",
        "                                                 dec_padding_mask)\n",
        "    \n",
        "    # select the last word from the seq_len dimension\n",
        "        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "    \n",
        "    # return the result if the predicted_id is equal to the end token\n",
        "        if predicted_id == targ_lang_tokenizer.word_index[\"<end>\"]:\n",
        "            return tf.squeeze(output, axis=0), attention_weights\n",
        "    \n",
        "    # concatentate the predicted_id to the output which is given to the decoder\n",
        "    # as its input.\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "\n",
        "def plot_attention_weights(attention, sentence, result, layer):\n",
        "    fig = plt.figure(figsize=(16, 8))\n",
        "  \n",
        "    sentence = inp_lang_tokenizer.encode(sentence)\n",
        "  \n",
        "    attention = tf.squeeze(attention[layer], axis=0)\n",
        "  \n",
        "    for head in range(attention.shape[0]):\n",
        "        ax = fig.add_subplot(2, 4, head+1)\n",
        "\n",
        "        # plot the attention weights\n",
        "        ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
        "\n",
        "        fontdict = {'fontsize': 10}\n",
        "\n",
        "        ax.set_xticks(range(len(sentence)+2))\n",
        "        ax.set_yticks(range(len(result)))\n",
        "\n",
        "        ax.set_ylim(len(result)-1.5, -0.5)\n",
        "\n",
        "        ax.set_xticklabels(\n",
        "            ['<start>']+[tokenizer_pt.decode([i]) for i in sentence]+['<end>'], \n",
        "            fontdict=fontdict, rotation=90)\n",
        "\n",
        "        ax.set_yticklabels([tokenizer_en.decode([i]) for i in result \n",
        "                            if i < tokenizer_en.vocab_size], \n",
        "                           fontdict=fontdict)\n",
        "\n",
        "        ax.set_xlabel('Head {}'.format(head+1))\n",
        "  \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def translate(sentence, plot=''):\n",
        "    result, attention_weights = evaluate(sentence)\n",
        "    predicted_sentence = ([targ_lang_tokenizer.index_word[i] for i in result.numpy()])  \n",
        "\n",
        "    print('Input: {}'.format(sentence))\n",
        "    print('Predicted translation: {}'.format(predicted_sentence))\n",
        "  \n",
        "    if plot:\n",
        "        plot_attention_weights(attention_weights, sentence, result, plot)\n",
        "        \n",
        "translate(\"good morning.\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: good morning.\n",
            "Predicted translation: ['<start>', '.', '.', '.', '.', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auzFjOt19lRA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "bd6749aa-2a4d-4659-a52b-1f99d7f04f9f"
      },
      "source": [
        "translate(\"how are you?\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: how are you?\n",
            "Predicted translation: ['<start>', '?', '?', '?', '?', '?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNTLCBqK9lOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaieUROp9lLm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25Ui_bLR9lGs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 33,
      "outputs": []
    }
  ]
}